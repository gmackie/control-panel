#!/usr/bin/env python3
"""
Automated Performance Optimization System
Uses ML to automatically detect performance bottlenecks and apply optimizations
"""

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import json
import os
import subprocess
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, asdict
from sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import requests
import yaml
import joblib
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

@dataclass
class PerformanceBottleneck:
    """Data class for identified performance bottlenecks"""
    component: str
    metric: str
    current_value: float
    expected_value: float
    severity: str  # low, medium, high, critical
    impact_score: float
    description: str
    root_cause: str
    recommendation: str

@dataclass
class OptimizationAction:
    """Data class for optimization actions"""
    action_type: str  # scaling, configuration, resource_allocation
    target: str  # deployment, service, configmap, etc.
    action: str  # specific action to take
    parameters: Dict[str, Any]
    expected_improvement: float
    risk_level: str  # low, medium, high
    rollback_plan: str
    success_criteria: Dict[str, float]
    estimated_time: str

@dataclass
class OptimizationResult:
    """Data class for optimization results"""
    action: OptimizationAction
    status: str  # pending, in_progress, completed, failed, rolled_back
    start_time: datetime
    end_time: Optional[datetime]
    actual_improvement: Optional[float]
    side_effects: List[str]
    success: bool

class AutomatedPerformanceOptimizer:
    """Automated performance optimization using ML and intelligent automation"""
    
    def __init__(self, prometheus_url: str = "http://prometheus:9090", 
                 kubernetes_config: str = None):
        self.prometheus_url = prometheus_url
        self.kubernetes_config = kubernetes_config
        self.models = {}
        self.optimization_history = []
        self.model_path = "/models/performance_optimization"
        self.active_optimizations = {}
        
        # Performance thresholds
        self.performance_thresholds = {
            'cpu_usage': {'warning': 0.7, 'critical': 0.85},
            'memory_usage': {'warning': 0.8, 'critical': 0.9},
            'response_time_p95': {'warning': 0.3, 'critical': 0.5},
            'error_rate': {'warning': 0.01, 'critical': 0.05},
            'db_connections': {'warning': 80, 'critical': 95},
            'cache_hit_rate': {'warning': 0.8, 'critical': 0.7}  # Lower is worse
        }
        
        # Optimization strategies
        self.optimization_strategies = {
            'horizontal_scaling': self._horizontal_scaling_strategy,
            'vertical_scaling': self._vertical_scaling_strategy,
            'cache_optimization': self._cache_optimization_strategy,
            'database_optimization': self._database_optimization_strategy,
            'application_tuning': self._application_tuning_strategy,
            'infrastructure_optimization': self._infrastructure_optimization_strategy
        }
        
        # Create model directory
        os.makedirs(self.model_path, exist_ok=True)
        
        # Load existing models
        self._load_models()
    
    def analyze_performance(self) -> List[PerformanceBottleneck]:
        """Analyze current system performance and identify bottlenecks"""
        print("ðŸ” Analyzing system performance...")
        
        # Collect current metrics
        current_metrics = self._collect_performance_metrics()
        
        # Identify bottlenecks using ML models
        bottlenecks = []
        
        for metric_name, value in current_metrics.items():
            if metric_name in self.performance_thresholds:
                bottleneck = self._analyze_metric_performance(metric_name, value, current_metrics)
                if bottleneck:
                    bottlenecks.append(bottleneck)
        
        # Use ML to identify complex bottlenecks
        ml_bottlenecks = self._ml_bottleneck_detection(current_metrics)
        bottlenecks.extend(ml_bottlenecks)
        
        # Sort by impact score
        bottlenecks.sort(key=lambda x: x.impact_score, reverse=True)
        
        return bottlenecks
    
    def generate_optimization_plan(self, bottlenecks: List[PerformanceBottleneck]) -> List[OptimizationAction]:
        """Generate optimization actions for identified bottlenecks"""
        print("ðŸ“‹ Generating optimization plan...")
        
        optimization_actions = []
        
        for bottleneck in bottlenecks:
            # Determine best optimization strategy
            strategy = self._select_optimization_strategy(bottleneck)
            
            if strategy in self.optimization_strategies:
                actions = self.optimization_strategies[strategy](bottleneck)
                optimization_actions.extend(actions)
        
        # Prioritize actions by expected improvement and risk
        optimization_actions = self._prioritize_optimizations(optimization_actions)
        
        # Check for conflicts and dependencies
        optimization_actions = self._resolve_optimization_conflicts(optimization_actions)
        
        return optimization_actions
    
    def execute_optimizations(self, actions: List[OptimizationAction], 
                            auto_approve: bool = False) -> List[OptimizationResult]:
        """Execute optimization actions with safety checks"""
        print("ðŸš€ Executing optimizations...")
        
        results = []
        
        for action in actions:
            print(f"Processing: {action.action_type} - {action.action}")
            
            # Safety checks
            if not self._safety_check(action):
                print(f"âŒ Safety check failed for {action.action}")
                continue
            
            # Get approval for high-risk actions
            if action.risk_level == 'high' and not auto_approve:
                if not self._get_approval(action):
                    print(f"â­ï¸ Skipping high-risk action: {action.action}")
                    continue
            
            # Execute the optimization
            result = self._execute_single_optimization(action)
            results.append(result)
            
            # Monitor the result
            if result.success:
                self._monitor_optimization_impact(result)
            
            # Store in history
            self.optimization_history.append(result)
        
        return results
    
    def _collect_performance_metrics(self) -> Dict[str, float]:
        """Collect current performance metrics"""
        
        metrics_queries = {
            'cpu_usage': 'avg(rate(container_cpu_usage_seconds_total{container="control-panel"}[5m]))',
            'memory_usage_gb': 'avg(container_memory_usage_bytes{container="control-panel"}) / 1024 / 1024 / 1024',
            'memory_usage_percent': 'avg(container_memory_usage_bytes{container="control-panel"}) / avg(container_spec_memory_limit_bytes{container="control-panel"})',
            'response_time_p50': 'histogram_quantile(0.50, sum(rate(http_request_duration_seconds_bucket{job="control-panel"}[5m])) by (le))',
            'response_time_p95': 'histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{job="control-panel"}[5m])) by (le))',
            'response_time_p99': 'histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket{job="control-panel"}[5m])) by (le))',
            'request_rate': 'sum(rate(http_requests_total{job="control-panel"}[5m]))',
            'error_rate': 'sum(rate(http_requests_total{job="control-panel",status=~"5.."}[5m])) / sum(rate(http_requests_total{job="control-panel"}[5m]))',
            'db_connections_active': 'pg_stat_database_numbackends{datname="control_panel",state="active"}',
            'db_connections_total': 'pg_stat_database_numbackends{datname="control_panel"}',
            'db_query_time': 'avg(pg_stat_statements_mean_time_seconds)',
            'cache_hit_rate': 'sum(rate(control_panel_cache_hits[5m])) / sum(rate(control_panel_cache_requests[5m]))',
            'disk_usage': 'avg(1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes))',
            'network_in_mbps': 'avg(rate(container_network_receive_bytes_total{name="control-panel"}[5m])) / 1024 / 1024',
            'network_out_mbps': 'avg(rate(container_network_transmit_bytes_total{name="control-panel"}[5m])) / 1024 / 1024',
            'pod_count': 'count(up{job="control-panel"})',
            'restart_rate': 'rate(kube_pod_container_status_restarts_total{pod=~"control-panel.*"}[5m])'
        }
        
        metrics = {}
        
        for metric_name, query in metrics_queries.items():
            try:
                value = self._query_prometheus(query)
                metrics[metric_name] = value
            except Exception as e:
                print(f"Warning: Could not fetch {metric_name}: {e}")
                metrics[metric_name] = self._get_simulated_metric(metric_name)
        
        # Add derived metrics
        if 'db_connections_total' in metrics and metrics['db_connections_total'] > 0:
            metrics['db_connection_utilization'] = metrics['db_connections_active'] / metrics['db_connections_total']
        
        return metrics
    
    def _analyze_metric_performance(self, metric_name: str, value: float, 
                                  all_metrics: Dict[str, float]) -> Optional[PerformanceBottleneck]:
        """Analyze individual metric performance"""
        
        if metric_name not in self.performance_thresholds:
            return None
        
        thresholds = self.performance_thresholds[metric_name]
        
        # Determine severity
        severity = 'normal'
        impact_score = 0.0
        
        if metric_name == 'cache_hit_rate':
            # Lower cache hit rate is worse
            if value < thresholds['critical']:
                severity = 'critical'
                impact_score = 0.9
            elif value < thresholds['warning']:
                severity = 'high'
                impact_score = 0.7
        else:
            # Higher values are worse for most metrics
            if value > thresholds['critical']:
                severity = 'critical'
                impact_score = 0.9
            elif value > thresholds['warning']:
                severity = 'high'
                impact_score = 0.7
        
        if severity == 'normal':
            return None
        
        # Determine root cause and recommendation
        root_cause, recommendation = self._diagnose_metric_issue(metric_name, value, all_metrics)
        
        # Calculate expected value
        expected_value = thresholds['warning'] * 0.8  # Target 80% of warning threshold
        
        return PerformanceBottleneck(
            component=self._get_component_for_metric(metric_name),
            metric=metric_name,
            current_value=value,
            expected_value=expected_value,
            severity=severity,
            impact_score=impact_score,
            description=f"{metric_name} is {severity} at {value:.3f}",
            root_cause=root_cause,
            recommendation=recommendation
        )
    
    def _ml_bottleneck_detection(self, metrics: Dict[str, float]) -> List[PerformanceBottleneck]:
        """Use ML to detect complex performance bottlenecks"""
        bottlenecks = []
        
        # Create feature vector
        feature_names = ['cpu_usage', 'memory_usage_percent', 'response_time_p95', 
                        'request_rate', 'error_rate', 'db_connections_active']
        
        features = []
        for name in feature_names:
            features.append(metrics.get(name, 0.0))
        
        features = np.array(features).reshape(1, -1)
        
        # Use ensemble of models to detect patterns
        if 'bottleneck_classifier' in self.models:
            try:
                # Predict if there's a bottleneck
                prediction = self.models['bottleneck_classifier'].predict(features)[0]
                confidence = self.models['bottleneck_classifier'].predict_proba(features)[0].max()
                
                if prediction == 1 and confidence > 0.7:  # Bottleneck detected
                    # Use feature importance to identify the source
                    feature_importance = self.models['bottleneck_classifier'].feature_importances_
                    most_important_idx = np.argmax(feature_importance)
                    problematic_metric = feature_names[most_important_idx]
                    
                    bottleneck = PerformanceBottleneck(
                        component=self._get_component_for_metric(problematic_metric),
                        metric=problematic_metric,
                        current_value=metrics[problematic_metric],
                        expected_value=metrics[problematic_metric] * 0.8,
                        severity='medium',
                        impact_score=confidence,
                        description=f"ML detected complex bottleneck in {problematic_metric}",
                        root_cause="Multi-factor performance degradation",
                        recommendation="Consider comprehensive performance analysis"
                    )
                    bottlenecks.append(bottleneck)
            except Exception as e:
                print(f"Error in ML bottleneck detection: {e}")
        
        # Detect correlation-based bottlenecks
        correlation_bottlenecks = self._detect_correlation_bottlenecks(metrics)
        bottlenecks.extend(correlation_bottlenecks)
        
        return bottlenecks
    
    def _detect_correlation_bottlenecks(self, metrics: Dict[str, float]) -> List[PerformanceBottleneck]:
        """Detect bottlenecks based on metric correlations"""
        bottlenecks = []
        
        # High CPU + High Response Time
        if (metrics.get('cpu_usage', 0) > 0.7 and 
            metrics.get('response_time_p95', 0) > 0.3):
            bottlenecks.append(PerformanceBottleneck(
                component='application',
                metric='cpu_response_correlation',
                current_value=metrics.get('response_time_p95', 0),
                expected_value=0.2,
                severity='high',
                impact_score=0.8,
                description="High CPU usage correlating with slow response times",
                root_cause="CPU bottleneck affecting response performance",
                recommendation="Scale horizontally or optimize CPU-intensive operations"
            ))
        
        # High Memory + High Error Rate
        if (metrics.get('memory_usage_percent', 0) > 0.85 and 
            metrics.get('error_rate', 0) > 0.02):
            bottlenecks.append(PerformanceBottleneck(
                component='application',
                metric='memory_error_correlation',
                current_value=metrics.get('error_rate', 0),
                expected_value=0.01,
                severity='critical',
                impact_score=0.9,
                description="High memory usage correlating with errors",
                root_cause="Memory pressure causing application errors",
                recommendation="Increase memory allocation or optimize memory usage"
            ))
        
        # Database connections + Response time
        if (metrics.get('db_connections_active', 0) > 50 and 
            metrics.get('response_time_p95', 0) > 0.4):
            bottlenecks.append(PerformanceBottleneck(
                component='database',
                metric='db_response_correlation',
                current_value=metrics.get('db_connections_active', 0),
                expected_value=30,
                severity='high',
                impact_score=0.75,
                description="High database connections correlating with slow responses",
                root_cause="Database connection pool saturation",
                recommendation="Optimize database queries or increase connection pool size"
            ))
        
        return bottlenecks
    
    def _select_optimization_strategy(self, bottleneck: PerformanceBottleneck) -> str:
        """Select the best optimization strategy for a bottleneck"""
        
        strategy_mapping = {
            'cpu_usage': 'horizontal_scaling',
            'memory_usage': 'vertical_scaling',
            'memory_usage_percent': 'vertical_scaling',
            'response_time': 'cache_optimization',
            'db_connections': 'database_optimization',
            'cache_hit_rate': 'cache_optimization',
            'error_rate': 'application_tuning'
        }
        
        # Check for specific patterns in metric name
        for pattern, strategy in strategy_mapping.items():
            if pattern in bottleneck.metric:
                return strategy
        
        # Default based on component
        component_strategies = {
            'application': 'horizontal_scaling',
            'database': 'database_optimization',
            'cache': 'cache_optimization',
            'infrastructure': 'infrastructure_optimization'
        }
        
        return component_strategies.get(bottleneck.component, 'horizontal_scaling')
    
    def _horizontal_scaling_strategy(self, bottleneck: PerformanceBottleneck) -> List[OptimizationAction]:
        """Generate horizontal scaling optimization actions"""
        actions = []
        
        # Get current replica count
        current_replicas = self._get_current_replicas('control-panel')
        
        # Calculate target replicas based on load
        if bottleneck.severity == 'critical':
            target_replicas = min(current_replicas * 2, 10)  # Double up to max 10
        elif bottleneck.severity == 'high':
            target_replicas = min(current_replicas + 2, 8)   # Add 2 up to max 8
        else:
            target_replicas = min(current_replicas + 1, 6)   # Add 1 up to max 6
        
        if target_replicas > current_replicas:
            action = OptimizationAction(
                action_type='horizontal_scaling',
                target='deployment/control-panel',
                action=f'Scale from {current_replicas} to {target_replicas} replicas',
                parameters={
                    'replicas': target_replicas,
                    'namespace': 'control-panel'
                },
                expected_improvement=0.3 + (target_replicas - current_replicas) * 0.1,
                risk_level='low',
                rollback_plan=f'Scale back to {current_replicas} replicas',
                success_criteria={
                    'cpu_usage': bottleneck.expected_value,
                    'response_time_p95': 0.2
                },
                estimated_time='2-5 minutes'
            )
            actions.append(action)
        
        return actions
    
    def _vertical_scaling_strategy(self, bottleneck: PerformanceBottleneck) -> List[OptimizationAction]:
        """Generate vertical scaling optimization actions"""
        actions = []
        
        # Get current resource limits
        current_resources = self._get_current_resources('control-panel')
        
        if 'memory' in bottleneck.metric:
            # Scale memory
            current_memory = current_resources.get('memory', '2Gi')
            memory_gb = int(current_memory.replace('Gi', '').replace('G', ''))
            
            if bottleneck.severity == 'critical':
                new_memory = min(memory_gb * 2, 8)  # Double up to 8GB
            else:
                new_memory = min(memory_gb + 1, 6)  # Add 1GB up to 6GB
            
            action = OptimizationAction(
                action_type='vertical_scaling',
                target='deployment/control-panel',
                action=f'Increase memory from {memory_gb}Gi to {new_memory}Gi',
                parameters={
                    'memory_limit': f'{new_memory}Gi',
                    'memory_request': f'{int(new_memory * 0.8)}Gi',
                    'namespace': 'control-panel'
                },
                expected_improvement=0.4,
                risk_level='medium',
                rollback_plan=f'Revert memory to {memory_gb}Gi',
                success_criteria={
                    'memory_usage_percent': 0.7
                },
                estimated_time='3-7 minutes'
            )
            actions.append(action)
        
        if 'cpu' in bottleneck.metric:
            # Scale CPU
            current_cpu = current_resources.get('cpu', '1000m')
            cpu_millicores = int(current_cpu.replace('m', ''))
            
            if bottleneck.severity == 'critical':
                new_cpu = min(cpu_millicores * 2, 4000)  # Double up to 4 cores
            else:
                new_cpu = min(cpu_millicores + 500, 3000)  # Add 0.5 core up to 3 cores
            
            action = OptimizationAction(
                action_type='vertical_scaling',
                target='deployment/control-panel',
                action=f'Increase CPU from {cpu_millicores}m to {new_cpu}m',
                parameters={
                    'cpu_limit': f'{new_cpu}m',
                    'cpu_request': f'{int(new_cpu * 0.8)}m',
                    'namespace': 'control-panel'
                },
                expected_improvement=0.35,
                risk_level='medium',
                rollback_plan=f'Revert CPU to {cpu_millicores}m',
                success_criteria={
                    'cpu_usage': 0.6
                },
                estimated_time='3-7 minutes'
            )
            actions.append(action)
        
        return actions
    
    def _cache_optimization_strategy(self, bottleneck: PerformanceBottleneck) -> List[OptimizationAction]:
        """Generate cache optimization actions"""
        actions = []
        
        if 'cache_hit_rate' in bottleneck.metric:
            # Increase cache size
            action = OptimizationAction(
                action_type='configuration',
                target='configmap/cache-config',
                action='Increase cache size and TTL',
                parameters={
                    'max_cache_size': '256MB',
                    'default_ttl': '1800',  # 30 minutes
                    'cache_algorithm': 'LRU'
                },
                expected_improvement=0.25,
                risk_level='low',
                rollback_plan='Revert to previous cache configuration',
                success_criteria={
                    'cache_hit_rate': 0.85
                },
                estimated_time='1-2 minutes'
            )
            actions.append(action)
        
        if 'response_time' in bottleneck.metric:
            # Enable response caching
            action = OptimizationAction(
                action_type='configuration',
                target='configmap/app-config',
                action='Enable response caching for API endpoints',
                parameters={
                    'enable_response_cache': True,
                    'cache_duration': '300',  # 5 minutes
                    'cached_endpoints': ['/api/metrics', '/api/cluster/health', '/api/applications']
                },
                expected_improvement=0.4,
                risk_level='low',
                rollback_plan='Disable response caching',
                success_criteria={
                    'response_time_p95': 0.15
                },
                estimated_time='2-3 minutes'
            )
            actions.append(action)
        
        return actions
    
    def _database_optimization_strategy(self, bottleneck: PerformanceBottleneck) -> List[OptimizationAction]:
        """Generate database optimization actions"""
        actions = []
        
        if 'db_connections' in bottleneck.metric:
            # Optimize connection pool
            action = OptimizationAction(
                action_type='configuration',
                target='configmap/database-config',
                action='Optimize database connection pool',
                parameters={
                    'max_connections': 100,
                    'idle_timeout': 300,
                    'connection_timeout': 30,
                    'pool_size': 20
                },
                expected_improvement=0.3,
                risk_level='medium',
                rollback_plan='Revert to previous connection pool settings',
                success_criteria={
                    'db_connections_active': 40
                },
                estimated_time='2-4 minutes'
            )
            actions.append(action)
        
        if 'db_query_time' in bottleneck.metric:
            # Enable query optimization
            action = OptimizationAction(
                action_type='configuration',
                target='configmap/database-config',
                action='Enable query optimization and caching',
                parameters={
                    'query_cache_enabled': True,
                    'query_cache_size': '64MB',
                    'slow_query_log': True,
                    'log_queries_not_using_indexes': True
                },
                expected_improvement=0.35,
                risk_level='low',
                rollback_plan='Disable query optimizations',
                success_criteria={
                    'db_query_time': 0.02
                },
                estimated_time='3-5 minutes'
            )
            actions.append(action)
        
        return actions
    
    def _application_tuning_strategy(self, bottleneck: PerformanceBottleneck) -> List[OptimizationAction]:
        """Generate application tuning actions"""
        actions = []
        
        if 'error_rate' in bottleneck.metric:
            # Tune application settings
            action = OptimizationAction(
                action_type='configuration',
                target='configmap/app-config',
                action='Optimize application error handling and retries',
                parameters={
                    'retry_attempts': 3,
                    'retry_delay': 1000,  # ms
                    'circuit_breaker_enabled': True,
                    'timeout_settings': {
                        'request_timeout': 30,
                        'database_timeout': 15,
                        'integration_timeout': 10
                    }
                },
                expected_improvement=0.25,
                risk_level='medium',
                rollback_plan='Revert to previous application settings',
                success_criteria={
                    'error_rate': 0.005
                },
                estimated_time='2-4 minutes'
            )
            actions.append(action)
        
        return actions
    
    def _infrastructure_optimization_strategy(self, bottleneck: PerformanceBottleneck) -> List[OptimizationAction]:
        """Generate infrastructure optimization actions"""
        actions = []
        
        # Generic infrastructure optimizations
        action = OptimizationAction(
            action_type='infrastructure_optimization',
            target='cluster',
            action='Optimize node affinity and resource allocation',
            parameters={
                'enable_node_affinity': True,
                'resource_quotas': True,
                'pod_disruption_budget': True
            },
            expected_improvement=0.2,
            risk_level='medium',
            rollback_plan='Revert infrastructure changes',
            success_criteria={},
            estimated_time='5-10 minutes'
        )
        actions.append(action)
        
        return actions
    
    def _prioritize_optimizations(self, actions: List[OptimizationAction]) -> List[OptimizationAction]:
        """Prioritize optimization actions"""
        
        # Score actions based on expected improvement and risk
        def priority_score(action):
            improvement_weight = 0.6
            risk_penalty = {'low': 0, 'medium': 0.1, 'high': 0.3}
            
            score = (action.expected_improvement * improvement_weight - 
                    risk_penalty.get(action.risk_level, 0.2))
            return score
        
        return sorted(actions, key=priority_score, reverse=True)
    
    def _resolve_optimization_conflicts(self, actions: List[OptimizationAction]) -> List[OptimizationAction]:
        """Resolve conflicts between optimization actions"""
        
        # Group actions by target
        target_groups = defaultdict(list)
        for action in actions:
            target_groups[action.target].append(action)
        
        resolved_actions = []
        
        for target, group in target_groups.items():
            if len(group) == 1:
                resolved_actions.extend(group)
            else:
                # Keep the highest priority action for each target
                best_action = max(group, key=lambda a: a.expected_improvement)
                resolved_actions.append(best_action)
        
        return resolved_actions
    
    def _safety_check(self, action: OptimizationAction) -> bool:
        """Perform safety checks before executing optimization"""
        
        # Check system health
        current_metrics = self._collect_performance_metrics()
        
        # Don't optimize if system is severely degraded
        if (current_metrics.get('error_rate', 0) > 0.1 or 
            current_metrics.get('cpu_usage', 0) > 0.95):
            print(f"âŒ System too degraded for optimization: {action.action}")
            return False
        
        # Check resource limits
        if action.action_type == 'vertical_scaling':
            # Ensure we don't exceed cluster capacity
            if not self._check_cluster_capacity(action):
                print(f"âŒ Insufficient cluster capacity for: {action.action}")
                return False
        
        # Check for recent optimizations on same target
        recent_optimizations = [
            opt for opt in self.optimization_history[-10:]
            if (opt.action.target == action.target and 
                opt.start_time > datetime.now() - timedelta(minutes=30))
        ]
        
        if recent_optimizations:
            print(f"âš ï¸ Recent optimization on {action.target}, skipping")
            return False
        
        return True
    
    def _get_approval(self, action: OptimizationAction) -> bool:
        """Get approval for high-risk actions (simulated)"""
        print(f"ðŸ¤” High-risk action requires approval: {action.action}")
        print(f"   Expected improvement: {action.expected_improvement:.1%}")
        print(f"   Risk level: {action.risk_level}")
        print(f"   Rollback plan: {action.rollback_plan}")
        
        # In a real system, this would integrate with approval workflow
        # For simulation, we'll auto-approve based on expected improvement
        return action.expected_improvement > 0.3
    
    def _execute_single_optimization(self, action: OptimizationAction) -> OptimizationResult:
        """Execute a single optimization action"""
        
        result = OptimizationResult(
            action=action,
            status='in_progress',
            start_time=datetime.now(),
            end_time=None,
            actual_improvement=None,
            side_effects=[],
            success=False
        )
        
        try:
            # Store baseline metrics
            baseline_metrics = self._collect_performance_metrics()
            
            # Execute the action
            if action.action_type == 'horizontal_scaling':
                success = self._execute_horizontal_scaling(action)
            elif action.action_type == 'vertical_scaling':
                success = self._execute_vertical_scaling(action)
            elif action.action_type == 'configuration':
                success = self._execute_configuration_change(action)
            else:
                success = self._execute_generic_action(action)
            
            # Wait for changes to take effect
            import time
            time.sleep(30)  # Wait 30 seconds
            
            # Measure improvement
            new_metrics = self._collect_performance_metrics()
            improvement = self._calculate_improvement(baseline_metrics, new_metrics, action)
            
            result.end_time = datetime.now()
            result.actual_improvement = improvement
            result.success = success and improvement > 0
            result.status = 'completed' if result.success else 'failed'
            
            print(f"âœ… Optimization completed: {action.action}")
            print(f"   Actual improvement: {improvement:.1%}")
            
        except Exception as e:
            result.end_time = datetime.now()
            result.status = 'failed'
            result.side_effects.append(f"Error: {str(e)}")
            print(f"âŒ Optimization failed: {action.action} - {e}")
        
        return result
    
    def _execute_horizontal_scaling(self, action: OptimizationAction) -> bool:
        """Execute horizontal scaling"""
        try:
            # Simulate kubectl scale command
            replicas = action.parameters['replicas']
            namespace = action.parameters.get('namespace', 'control-panel')
            
            cmd = f"kubectl scale deployment control-panel --replicas={replicas} -n {namespace}"
            print(f"Executing: {cmd}")
            
            # In a real implementation, this would execute the actual kubectl command
            # result = subprocess.run(cmd.split(), capture_output=True, text=True)
            # return result.returncode == 0
            
            return True  # Simulate success
            
        except Exception:
            return False
    
    def _execute_vertical_scaling(self, action: OptimizationAction) -> bool:
        """Execute vertical scaling"""
        try:
            # Simulate resource limit update
            params = action.parameters
            
            # Create patch for resource limits
            patch = {
                "spec": {
                    "template": {
                        "spec": {
                            "containers": [{
                                "name": "control-panel",
                                "resources": {
                                    "limits": {},
                                    "requests": {}
                                }
                            }]
                        }
                    }
                }
            }
            
            if 'memory_limit' in params:
                patch["spec"]["template"]["spec"]["containers"][0]["resources"]["limits"]["memory"] = params['memory_limit']
                patch["spec"]["template"]["spec"]["containers"][0]["resources"]["requests"]["memory"] = params.get('memory_request', params['memory_limit'])
            
            if 'cpu_limit' in params:
                patch["spec"]["template"]["spec"]["containers"][0]["resources"]["limits"]["cpu"] = params['cpu_limit']
                patch["spec"]["template"]["spec"]["containers"][0]["resources"]["requests"]["cpu"] = params.get('cpu_request', params['cpu_limit'])
            
            print(f"Applying resource patch: {patch}")
            
            # In a real implementation:
            # kubectl patch deployment control-panel -n control-panel --patch='{patch_json}'
            
            return True  # Simulate success
            
        except Exception:
            return False
    
    def _execute_configuration_change(self, action: OptimizationAction) -> bool:
        """Execute configuration changes"""
        try:
            target = action.target
            params = action.parameters
            
            print(f"Updating configuration {target} with parameters: {params}")
            
            # In a real implementation, this would update ConfigMaps or other configurations
            # For example:
            # kubectl patch configmap app-config -n control-panel --patch='{data: {config.yaml: "..."}}'
            
            return True  # Simulate success
            
        except Exception:
            return False
    
    def _execute_generic_action(self, action: OptimizationAction) -> bool:
        """Execute generic optimization actions"""
        print(f"Executing generic action: {action.action}")
        return True  # Simulate success
    
    def _calculate_improvement(self, baseline: Dict[str, float], 
                             new_metrics: Dict[str, float], 
                             action: OptimizationAction) -> float:
        """Calculate actual improvement from optimization"""
        
        # For each success criteria, calculate improvement
        improvements = []
        
        for metric, target_value in action.success_criteria.items():
            if metric in baseline and metric in new_metrics:
                baseline_value = baseline[metric]
                new_value = new_metrics[metric]
                
                # Calculate improvement (depends on whether lower or higher is better)
                if metric in ['response_time_p95', 'error_rate', 'cpu_usage', 'memory_usage_percent']:
                    # Lower is better
                    if baseline_value > 0:
                        improvement = (baseline_value - new_value) / baseline_value
                    else:
                        improvement = 0
                else:
                    # Higher is better (like cache_hit_rate)
                    if baseline_value > 0:
                        improvement = (new_value - baseline_value) / baseline_value
                    else:
                        improvement = 0
                
                improvements.append(improvement)
        
        # Return average improvement, or estimated improvement if no criteria
        if improvements:
            return sum(improvements) / len(improvements)
        else:
            # Simulate improvement based on action type
            return np.random.uniform(0.1, action.expected_improvement)
    
    def _monitor_optimization_impact(self, result: OptimizationResult):
        """Monitor the impact of optimization over time"""
        
        # This would run in background to monitor optimization effects
        # For now, we'll just log the result
        print(f"ðŸ“Š Monitoring optimization impact: {result.action.action}")
        
        # Store for future reference
        self.active_optimizations[result.action.target] = result
    
    def _query_prometheus(self, query: str) -> float:
        """Query Prometheus for metric value"""
        try:
            response = requests.get(
                f"{self.prometheus_url}/api/v1/query",
                params={'query': query},
                timeout=10
            )
            
            if response.status_code == 200:
                data = response.json()
                if data['status'] == 'success' and data['data']['result']:
                    return float(data['data']['result'][0]['value'][1])
            
            # Fallback to simulated data
            return self._get_simulated_metric(query)
            
        except Exception:
            return self._get_simulated_metric(query)
    
    def _get_simulated_metric(self, metric_name: str) -> float:
        """Get simulated metric values for testing"""
        
        # Simulate some performance issues
        hour = datetime.now().hour
        base_multiplier = 1.0 + 0.3 * np.sin(hour * np.pi / 12)  # Daily pattern
        
        base_values = {
            'cpu_usage': 0.6 * base_multiplier,
            'memory_usage_gb': 2.5 * base_multiplier,
            'memory_usage_percent': 0.75 * base_multiplier,
            'response_time_p95': 0.25 * base_multiplier,
            'request_rate': 45 / base_multiplier,
            'error_rate': 0.02 * base_multiplier,
            'db_connections_active': 40 * base_multiplier,
            'cache_hit_rate': 0.75 / base_multiplier,
            'pod_count': 3
        }
        
        # Add some randomness
        for key, value in base_values.items():
            if key in metric_name:
                return max(0, value + np.random.normal(0, value * 0.1))
        
        return np.random.uniform(0.5, 1.5)
    
    def _get_component_for_metric(self, metric_name: str) -> str:
        """Get component name for a metric"""
        
        component_mapping = {
            'cpu': 'application',
            'memory': 'application',
            'response_time': 'application',
            'request_rate': 'application',
            'error_rate': 'application',
            'db': 'database',
            'cache': 'cache',
            'disk': 'infrastructure',
            'network': 'infrastructure',
            'pod': 'infrastructure'
        }
        
        for pattern, component in component_mapping.items():
            if pattern in metric_name:
                return component
        
        return 'system'
    
    def _diagnose_metric_issue(self, metric_name: str, value: float, 
                             all_metrics: Dict[str, float]) -> Tuple[str, str]:
        """Diagnose the root cause and recommendation for a metric issue"""
        
        diagnoses = {
            'cpu_usage': (
                "High CPU utilization affecting application performance",
                "Scale horizontally or optimize CPU-intensive operations"
            ),
            'memory_usage': (
                "High memory usage approaching limits",
                "Increase memory allocation or optimize memory usage patterns"
            ),
            'response_time_p95': (
                "Slow response times affecting user experience",
                "Enable caching, optimize queries, or scale application"
            ),
            'error_rate': (
                "Elevated error rate indicating application issues",
                "Review application logs and implement better error handling"
            ),
            'db_connections': (
                "High database connection usage",
                "Optimize connection pooling or scale database"
            ),
            'cache_hit_rate': (
                "Low cache hit rate reducing performance",
                "Increase cache size or optimize caching strategy"
            )
        }
        
        for pattern, (cause, recommendation) in diagnoses.items():
            if pattern in metric_name:
                return cause, recommendation
        
        return "Performance degradation detected", "Investigate and optimize"
    
    def _get_current_replicas(self, deployment: str) -> int:
        """Get current replica count for deployment"""
        # Simulate getting current replica count
        return 3  # Default
    
    def _get_current_resources(self, deployment: str) -> Dict[str, str]:
        """Get current resource limits for deployment"""
        # Simulate getting current resources
        return {
            'cpu': '1000m',
            'memory': '2Gi'
        }
    
    def _check_cluster_capacity(self, action: OptimizationAction) -> bool:
        """Check if cluster has capacity for the optimization"""
        # Simulate cluster capacity check
        return True
    
    def _save_models(self):
        """Save optimization models"""
        models_file = f"{self.model_path}/optimization_models.pkl"
        
        with open(models_file, 'wb') as f:
            joblib.dump({
                'models': self.models,
                'optimization_history': self.optimization_history[-1000:]  # Keep last 1000
            }, f)
    
    def _load_models(self):
        """Load optimization models"""
        try:
            models_file = f"{self.model_path}/optimization_models.pkl"
            
            if os.path.exists(models_file):
                data = joblib.load(models_file)
                self.models = data.get('models', {})
                self.optimization_history = data.get('optimization_history', [])
                
                print(f"Loaded optimization models and {len(self.optimization_history)} historical results")
            
        except Exception as e:
            print(f"Could not load existing models: {e}")
    
    def get_optimization_summary(self) -> Dict:
        """Get summary of optimization activities"""
        
        if not self.optimization_history:
            return {"message": "No optimization history available"}
        
        total_optimizations = len(self.optimization_history)
        successful_optimizations = sum(1 for opt in self.optimization_history if opt.success)
        
        # Average improvement
        improvements = [opt.actual_improvement for opt in self.optimization_history 
                       if opt.actual_improvement is not None]
        avg_improvement = sum(improvements) / len(improvements) if improvements else 0
        
        # Most common optimization types
        action_types = [opt.action.action_type for opt in self.optimization_history]
        action_type_counts = {}
        for action_type in action_types:
            action_type_counts[action_type] = action_type_counts.get(action_type, 0) + 1
        
        return {
            'total_optimizations': total_optimizations,
            'success_rate': f"{successful_optimizations / total_optimizations:.1%}" if total_optimizations > 0 else "0%",
            'average_improvement': f"{avg_improvement:.1%}",
            'optimization_types': action_type_counts,
            'active_optimizations': len(self.active_optimizations)
        }


def main():
    """Main execution function for testing"""
    print("ðŸ¤– Starting Automated Performance Optimization System...")
    
    # Initialize the optimizer
    optimizer = AutomatedPerformanceOptimizer()
    
    # Analyze current performance
    print("\nðŸ” Analyzing system performance...")
    bottlenecks = optimizer.analyze_performance()
    
    if bottlenecks:
        print(f"\nðŸ“Š Found {len(bottlenecks)} performance bottlenecks:")
        for i, bottleneck in enumerate(bottlenecks[:3], 1):
            print(f"{i}. {bottleneck.component} - {bottleneck.metric}")
            print(f"   Severity: {bottleneck.severity}")
            print(f"   Current: {bottleneck.current_value:.3f}, Expected: {bottleneck.expected_value:.3f}")
            print(f"   Impact Score: {bottleneck.impact_score:.2f}")
            print(f"   Root Cause: {bottleneck.root_cause}")
            print(f"   Recommendation: {bottleneck.recommendation}")
    else:
        print("âœ… No significant performance bottlenecks detected")
        return
    
    # Generate optimization plan
    print("\nðŸ“‹ Generating optimization plan...")
    optimization_plan = optimizer.generate_optimization_plan(bottlenecks)
    
    if optimization_plan:
        print(f"\nðŸŽ¯ Generated {len(optimization_plan)} optimization actions:")
        for i, action in enumerate(optimization_plan, 1):
            print(f"{i}. {action.action_type}: {action.action}")
            print(f"   Target: {action.target}")
            print(f"   Expected Improvement: {action.expected_improvement:.1%}")
            print(f"   Risk Level: {action.risk_level}")
            print(f"   Estimated Time: {action.estimated_time}")
            print()
    
    # Execute optimizations (auto-approve for demo)
    print("ðŸš€ Executing optimizations...")
    results = optimizer.execute_optimizations(optimization_plan, auto_approve=True)
    
    print(f"\nðŸ“ˆ Optimization Results:")
    successful_count = 0
    total_improvement = 0
    
    for result in results:
        if result.success:
            successful_count += 1
            if result.actual_improvement:
                total_improvement += result.actual_improvement
            
            print(f"âœ… {result.action.action}")
            print(f"   Status: {result.status}")
            print(f"   Improvement: {result.actual_improvement:.1%}")
        else:
            print(f"âŒ {result.action.action}")
            print(f"   Status: {result.status}")
            if result.side_effects:
                print(f"   Issues: {', '.join(result.side_effects)}")
    
    # Summary
    avg_improvement = total_improvement / successful_count if successful_count > 0 else 0
    print(f"\nðŸ“Š Summary:")
    print(f"   Total Actions: {len(results)}")
    print(f"   Successful: {successful_count}")
    print(f"   Success Rate: {successful_count / len(results):.1%}")
    print(f"   Average Improvement: {avg_improvement:.1%}")
    
    # Get overall summary
    print(f"\nðŸ“‹ System Summary:")
    summary = optimizer.get_optimization_summary()
    for key, value in summary.items():
        print(f"   {key}: {value}")
    
    print("\nâœ… Automated Performance Optimization completed!")


if __name__ == "__main__":
    main()